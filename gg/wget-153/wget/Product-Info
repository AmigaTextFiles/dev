.name
wget
.type
networking tool
.short
retrieve files over net via HTTP and FTP
.description
Wget [formerly known as Geturl] is a freely available network utility
to retrieve files from the World Wide Web using HTTP and FTP, the two
most widely used Internet protocols.  It works non-interactively, thus
enabling work in the background, after having logged off.

The recursive retrieval of HTML pages, as well as FTP sites is
supported -- you can use Wget to make mirrors of archives and home
pages, or traverse the web like a WWW robot (Wget understands
/robots.txt).

Wget works exceedingly well on slow or unstable connections, keeping
getting the document until it is fully retrieved. Re-getting files
from where it left off works on servers (both HTTP and FTP) that
support it. Matching of wildcards and recursive mirroring of
directories are available when retrieving via FTP. Both HTTP and FTP
retrievals can be time-stamped, thus Wget can see if the remote file
has changed since last retrieval and automatically retrieve the new
version if it has.

Wget supports proxy servers, to lighten the network load and to speed
up retrieval. If you are behind a firewall and have the socks library
installed, you can compile wget with support for socks.

Most of the features are configurable, either through command-line
options, or via initialization file .wgetrc.  Wget allows you to
install a global startup file (/usr/local/lib/wgetrc by default) for
site settings.

Wget works under almost all modern Unix variants and, unlike many
other similar utilities, is written entirely in C, thus requiring no
additional software (like perl). As Wget uses the GNU Autoconf, it is
easily built on and ported to other Unix's. Installation procedure is
described in the INSTALL file.
.version
1.5.2
.author
Hrvoje Niksic
.distribution
GNU GPL
.email
hniksic@srce.hr
.described-by
Fred Fish (fnf@ninemoons.com)
.submittal
Downloaded via ftp from gnu.ai.mit.edu
