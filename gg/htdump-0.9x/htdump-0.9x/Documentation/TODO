htdump is actually a program I wrote as part of a
bigger whole, namely a website copy program. There
are plenty of programs like that for Windows, like
Teleport, Websnake and such. However both programs
have various shortcommings. Teleport doesn't do
ftp en if files get disconnected they don't get
resumed. Websnake has no option to stop the
process and resume later, so the following
programs are still on the list:

 * 'htlink' to extract URLs
 * 'htleech' a manager that uses htlink and htdump
 * 'htlocalise' to localise all html pages


TODO's for htdump itself:

 * Some kind of handling according to errorcode
 * 'keep-alive' connections
 * 'chunked' transfer coding (file uploading)
 * Checking file date on server and local, update if needed
 * Escape all special characters in the URL, like '#'s or we get a 'not found'..
 * Need to write a man page.. anybody know how one does that?
 * Add Keep-Alive connections support
 * Add detailed elapsed time and percentage complete info
 * Show in the process list, percentage complete and such.
   However this only seems to work if I fork to the background.
 * Some cookie management of sorts. Maybe keep a file with cookies.
 * When the server gives back a document with 'Transfer-Encoding: chunked'
   then there's some stuff at the beginning and end of the data..
   Have to figure out what to do about that.
 * When resuming a file, check for code '206 Partial Content'
