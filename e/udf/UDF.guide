@database UDF.guide

@AUTHOR Mauro "M&F" Fontana
@$VER:  UDF guide v0.8E
@WIDTH 75

@node main UDF

                             @{b}    UDF 0.8E @{ub}
                            
                             AmigaE only version

  @{" " LINK "contents"} Contents od the archive
  @{" " LINK "intro"} Introduction
  @{" " LINK "format"} Format description
  @{" " LINK "usage"} Usage
  @{" " LINK "criteria"} Finding criteria
  @{" " LINK "chunk"} Creating new chunk
  @{" " LINK "meta"} What are metachunk?
  @{" " LINK "metachunk"} Creating metachunk
  
  @{" " LINK hints} Hints
  
  @{" " LINK "hystory"} Hystory
  
@endnode

@node intro "Introduction"

                            @{b}Introduction@{ub}

  You  may  be interested why I wrote it. The story is simple: I don't like
  the excessive limitations of IFF, expecially if used as an archive format
  to contain lots of elements. IFF has been on the market for lots of years
  now,  an  though its basic idea is still valid, its implementation is not
  that  good. This is probably why IFF is widely used only for the standard
  ILBM  picture format (IFF and ILBM meanings are often mixed up). The only
  other  usage  is provided by the OS through the use of the clipboard. IFF
  handling  is  quite  complex  through  the  use  of  the both the iff and
  iffparse libraries. While IFF is good to be used for anything that has to
  be  accessed  "as a stream", where you just get data one after the other,
  it  shows  its limitations when used as a random access format, where you
  try  to  extract  just  parts  from  it. It is very difficult to find the
  needed  part  with  the  provided  functions, and worse, it is still more
  difficult  to handle data in a hierachical order. For all these reasons I
  created  this  archiving format, which is similar to the IFF as idea, but
  tries to go beyond its limitations.
  
  UDF stands for Universal Data Format. This means UDF archives can contain
  any  kind of data organized as the user wants it. Data is subdivided into
  chunks,  which are organized into separate levels in order to support for
  arbitrary  deep  nested  hierarchial  trees.  In  fact, each part, called
  chunk,  chunk  can have its own sub-chunks, and sub-chunks can have other
  sub-chunks, with no hardcoded limitations.
  
  Particular   attention  has  been  spent,  since  the  beginning  of  the
  development,  to make data manipulation easy. This implies that functions
  for  easy  find  the wanted chunk in the archive have been provided. Each
  chunk  has  a  header  that provides the necessary information to make it
  unique  inside the archive. Combining the right values for the parameters
  to  match  makes extracting the desired chunk(s) very easy. Chunks can be
  described by their id, size, number, mask or number of subchunks.
  
  In  UDF  files chunks are all threated the same way, that is there are no
  special chunks for special uses. This means information for chunk headers
  are  completely  at user will. No check or limitation on them is provided
  by  the  format. Of course such information should be understable, or the
  easy  of  use advantage will be lost. For instance, the format allows you
  to  store an image in a chunk which is set as being a text chunk. However
  this  would  confuse anyone trying to access data in such a chunk. If you
  want  to  keep your data meaning private, you can use one of the privided
  general use ids or create your own custom id as explained later.

  A well organized archive may allow for easy reaching of chunks that works
  as  containers  for  particular kind of other chunks. For example a chunk
  can be the parent of a group of chunks of the same type.

  Creation  of  archives  is very simple: you can add chunks on the current
  level,  or you can create a new sublevel for the last created chunk. Once
  the archive is ready you just need to save it.
  
  Reading  an  archive is simpler, as you just need to load it and the tree
  of chunks is automatically recreated in memory. It is then very simple to
  obtain the desired chunk as said before.

  To  avoid an easy "browsing" among the data of an archive through any HEX
  editor,  simpler  and fast custom functions have been provided to encrypt
  data.  If  more  power  is  nedeed,  compression  though XPK libraries is
  provided  as  well.  This  should  stop people reading, for example, text
  chunks in the archive with a simple HEX editor.
  
  This format is still being developed, but as it stands now it has already
  many  advantages with respect to the IFF standard. And more will be added
  as features are added, thanks to the fact that it has been completely OOP
  coded.
  
@endnode


@node format "Description of the format"

                         @{b}Format description@{ub}
  
  This section describes the internal organization of data into memory. It
  may interests you, but you are not required to read it if it doesn't.
  
  The  Universal  Data  Format  is  all  handled  though  an OOP class. All
  internal information and data organization of the UDF class has been kept
  hidden,  to avoid any try of manual maniplation of data. The provided API
  is  the  only  way  to  make things the right way and future enhancements
  proof.
  
  The  UDF  object  behaves  quite  differently  depending  whether you are
  creating an archive or you are trying to access it for extracting data. A
  limitation  that  exists  (but may be removed in future) is the fact that
  loaded  archives cannot be modified. The only way to modify an archive is
  to build it from scratch, even if starting from an already existing one.
  
  However  the  basic  elements consituting the UDF archive are the same: a
  main UDF object, and a linked list of chunks objects. From the UDF object
  you  can  have  a  list  of current chunks in the archive. The chunks are
  organized  as  a tree, that is they are divided into separated levels and
  these  levels  are  linked  together  as  a single linked list. Available
  chunks  are considered all those that can be reached through the starting
  chunk  (header  of  the tree) by descending any needed level. Chunks that
  cannot be reached by starting from the header are considered "lost", that
  is,  they  are actually not part of the archive. There's not legal way to
  access  them.  Such  "lost"  chunks can be created in two ways. One is by
  adding chunks to the same level as the header (which will be prevented in
  future).  The  second way is by creating them "manually" by not using the
  provided API functions. "Lost" chunks are just a waste of memory and disk
  space as they are loaded but are not part of the hierarchical tree.

  A new level is created by adding sub-chunks to an already existing chunk.
  Such  sub-chunks,  which  are  normal chunks linked to the parent one, on
  their turn, can have other sub-chunks, and so on infinitely (or as memory
  allows).  There  are  only  two  rules that describe the behaviour of the
  chunk object:
  
  1) A chunk has access only to its own data and subchunks.
  
  2) Chunks can be linked in a list only if they have a parent chunk.

  These rules are simple and allows for easy manipulation of chunks, though
  they impose a single limitation: the very first level of the tree must be
  made  up  of  only  a  single  chunk.  All  other chunks must be added as
  sub-chunks  of  this  "main"  one. This explains why all chunks must, and
  can, be accessed starting from the first one.
  
  Level  access is one way only, that is, you can only enter a level from a
  level which is "above" it in the hierarchy. This however is not a problem
  as  the  API provides for functions that automatically scan thoughout the
  entire  tree  (or  from  a deetermined point in the tree) looking for the
  desired chunk.
  
  When  a  series  of  chunks  is "extracted" from the tree, that is when a
  series  of  chunks  satisfies the criteria provided for the search, it is
  presented  as  a  single linked list of nodes containing a pointer to the
  desired  chunks.  These  chunks  are  not  removed from the tree, neither
  modified.  This  simplifies  the handling of chunks a lot, as the tree is
  never  modified  but only lists of pointers to chunks are returned. These
  lists  are standard exec lists and have to be disposed of "manually" when
  you  have  finished  using them. The API provides functions also for this
  (see @{"usage" LINK usage}.


@endnode


@node usage "Usage"
  
                               @{b}Usage@{ub}

  An  API  (Application  Program  Interface)  has  been provided to use UDF
  files.  This API is the only, legal, way to access the data stored in the
  UDF  archives.  It  is also the only, legal, way to create such archives.
  "Hand   made"   archives,  or  manual  manipulation  of  UDF  files,  are
  deprecated,  and  everything  will be done to prevent that. All necessary
  features of the UDF format can be reached only through the provided API.

  There  are  two  ways  of  using  the  UDF object: one is for creating an
  archive,  the  other  is  to  load  the  archive  and look for the needed
  information. The functions are all methods of the UDF class and of the CK
  (chunk)  class,  and  as methods of such objects they have to be invoked.
  If  you  do  not  know how to use OOP classes you should avoid using this
  module and waiting for the standard shared library version of it.
  
  @{b}Creating an archive@{ub}
  
  These are the functions provided to create an archive from scratch.
  
  UDF methods:
  
  - @{b}new(filename)@{ub}: creates a new UDF object, initialising all its
  fields with the right standard values and setting its name as the
  provided argument.
  
  - @{b}end()@{ub}: dispose of the object and of all the relative allocated
  memory.
  
  - @{b}pushlevel(taglist)@{ub}: adds a chunk, passed as taglist argument,
  as a subchunk of the last created chunk. Creates a new level. Returns the
  pointer of the just created chunk. See @{"chunk creation" LINK "chunk"}
  for the supported tags.

  - @{b}pushchunk(taglist)@{ub}: add the provided chunk on the same level
  as the last created chunk. Returns the pointer of the just created chunk.
  See @{"chunk creation" LINK "chunk"} for the supported tags.

  - @{b}poplevel()@{ub}: exit current level and return to the parent one.
  From here new chunk can be added to the current level.
  
  - @{b}popchunk()@{ub}: finishes current chunk. This is automatically
  called when needed by pushchunk()/poplevel(), so you can always omit it.
  
  - @{b}save@{ub}: freezes the current tree and save it on disk as specified
  by the name given in new().

  - @{b}store(data, length)@{ub}: writes the supplied data into the last
  created chunk data buffer. length is optional and when missing the data
  is treathed as a string (that is it @{b}must@{ub} be \0 terminated) and
  its length is calculated automatically. If data is not a string length
  must be supplied (or strange things may happen).

  CK methods:
  
  - @{b}store(data, length)@{ub}: the same as above, but this time it
  writes the data in the chunk calling this methods (usually that returned
  by pushlevel()/pushchunk(), see above for more info).
  
  
  Notes about chunk creation: a chunk exists as a modifiable objects untill
  you  pop  it.  Popping  a  chunk  is  done,  even automatically, when you
  pushchunk()  and  poplevel(). After that, the chunk is in a "frozen" mode
  and  cannot be modified. Moreover any pointer to it is invalid, so if you
  try  to  use  a  popped  chunk to add further data may result in unwanted
  behaviour.  For  this  reason it is always recommended to use UDF store()
  methods that knows which is the last available chunk or use chunk store()
  method  only  with  the  pointer  returned  by  the  last  pushchunk() or
  pushlevel() call. The general rule is that you have always to modify only
  the last created chunk before it is "frozen".  
  
  
  @{b}Loading an archive and look for needed chunk(s)@{ub}
  
  These are the functions provided to handle an UDF archive.

  UDF methods:
  
  - @{b}new(filename)@{ub}: creates a new UDF object, initialising all its
  fields with the right standard values and setting its name as the
  provided argument.
  
  - @{b}end()@{ub}: dispose of the object and of all the relative allocated
  memory.

  - @{b}load()@{ub}: load the archivee file in memory. The name of the file
  to load is specified with new(). The hierarchy tree is created from the
  loaded information.
  
  - @{b}getversion()@{ub}: returns the version (and revision as secondary
  returned value) numbers for the loaded UDF file. These numbers indicates
  which version of the module was used to create the file. This may be
  useful in future when different versions may behave differently.
  
  - @{b}getsize()@{ub}: returns the size in bytes of the archive (which is
  the size of the file less the UDF header information).
  
  - @{b}getname()@{ub}: returns a pointer to the string containing the
  filename of the current udf object.
  
  - @{b}gethead()@{ub}: returns a pointer to the first chunk object. From
  this chunk all other chunks can be accessed.
  
  - @{b}countchunks(chunk)@{ub}: returns the number of all subchunks (and
  all their subchunks) from the supplied chunk to the end of the tree. If
  the chunk is omitted the counting starts from the first chunk in the
  tree.
  
  - @{b}findchunk(taglist, chunk)@{ub}: returns a pointer to the first
  chunk satisfing the required parameters starting from the supplied chunk.
  If this is missing, the search is started from the beginning of the tree.
  If no chunk matches, NIL is returned. Taglist contains the wanted
  parameters and their values to match. See @{"here" LINK criteria} for the
  allowed tags.
  
  - @{b}collectchunks(taglist, chunk)@{ub}: returns a linked list of nodes
  containing simply a pointer to the chunk satisfing the required
  paramters. The search for matching chunks is started from the eventual
  supplied chunk, otherwise from the beginning of the tree. The returned
  list may be an empty list if no chunk matches. See @{"here" LINK
  criteria} for the allowed tags.
  
  CK methods:
  
  - @{b}getdatabuffer()@{ub}: returns the pointer to the data buffer of the
  chunk. Please note that is is illegal to directly modify the data
  contained in this archive! This buffer is READ ONLY.
  
  - @{b}getsublist()@{ub}: returns the list of subchunks belonging to this
  chunk. Do not modify this list!
  
  - {b}getnumsubck()@{ub}: returns the number of subchunks belonging to
  this chunk.
  
  - @{b}getid()@{ub}: returns the id value of the this chunk.

  - @{b}getsize()@{ub}: returns the size of the current chunk data buffer
  (subchunks excluded).
  
  - @{b}getnumber()@{ub}: gets the number of this chunk.

  Any other method which is not described here has to be considered
  private, untill the E compiler does not allows for private methods (so
  you'll not know there are other methods 8) ).
  
  META CHUNK HANDDLING
  
@endnode

@node criteria "Searching parameters"

                        @{b}Searching parameters@{ub}

  These are the allowed tags that can be supplied in order to look for a
  desired chunk.

  - CK_ID, id_value (ID_PICTURE,ID_TEXT,ID_SOUND,....)
  - CK_SIZE, chunk_size
  - CK_NUMBER, chunk_number,
  - CK_MASK, mask value (MASK_NONE, MASK_XOR, MASK_SWAP, MASK_FAST)
  - CK_SUBCK, number_of_subchunk

  See their meaning in the @{"chunk creation" LINK "chunk"} section

@endnode

@node chunk "Tags for creation of new chunks"

                  @{b}Tags for creation of new chunks@{ub}

    These are the allowed tags to be used when creating a new chunk. Tags
    not set are intended as zero or NIL.

  - @{b}CK_ID@{ub}, id of the chunk.
    
    The provided IDs are:
    
    ID_PICTURE
    ID_TEXT
    ID_SOUND
    ID_TAGLIST
    ID_SHAPE
    ID_DOCUMENT
    ID_DATE
    ID_VERSION
    ID_RAW
    ID_XY
    ID_BOX
    ID_RGBA_COLOUR
    ID_CMYB_COLOUR
    
    ID_FOLDER
    ID_LIST
    ID_COMPLEX

    You can create your own custom ID by starting from the provided
    CUSTOM_ID_BASE value. Allowed HEX range is 0-7FFF, or dec 0-32767 which
    should be enough. All other values are reserved.
  
    Note that the name of these ID are not relevant at all for the
    automated functions performed inside UDF. They are all treated the same
    way. However for a better undestanding of the meaning of the archive
    from a user point of view it should be better using the relative ID for
    a certain kind of data. In particular, chunks used as containers for
    other chunks should be marked as one of the last three.
  
    The meaning of the provided IDs should be easy to understand, but maybe
    it is better to describe how should be organized the chunk content when
    marked as one of them:
    
    id value      |  chunk content
    --------------+------------------------------
    ID_PICTURE    | image of any format.
    ID_TEXT       | text with \0 at its end
    ID_SOUND      | any sound format
    ID_TAGLIST    | coupled longs as tagitems
    ID_SHAPE      | a string containing a shape name followed by the
                  | pameters needed to create it
    ID_DOCUMENT   | a formatted text
    ID_DATE       | a date string
    ID_VERSION    | a long containing a version number followed by a long
                  | containing a revision number
    ID_RAW        | anything which is not organized
    ID_XY         | two longs containing two values
    ID_BOX        | four longs containing the X, Y offsets and the width
                  | and height values
    ID_RGBA_COLOUR| four longs containing the red, green, blue and alpha
                  | values
    ID_CMYB_COLOUR| four longs containing the cyan, magenta, yellow,
                  | black values
    
    ID_FOLDER     | a generic folders for various type subchunks
    ID_LIST       | when subchunks are all of the same type
    ID_COMPLEX    | when subchunks are different but are repeated
                  | periodically (record structures)
    
    These are only the suggested way on how to organize the internal chunks
    data when they have such IDs. A user which interprets them this way
    should be always right, while building chunks with such IDs but
    different meaning is discouraged. Please use custom IDs when none of
    the above satisfy your needs. Following these simple rules will make
    sharing standard UDF archives very simple. You are always free to add
    metachunks to add custom/private or nor specific information to the
    chunk contents.
    
    
  - @{b}CK_SIZE@{ub}, if set, the size of the chunk is fixed whatever you
    store in it (trying to store more data is safe, but the new data will
    be lost). If SIZE_UNKNOWN is passed as value, the buffer is enlarged as
    needed while data are stored in it (no limit to the size of the data
    buffer but memory limits).
  
  - @{b}CK_NUMBER@{ub}, a 32 bit number useable to distinguish between
    chunks with the same ID. Useful for sorting chunks.
  
  - @{b}CK_MASK@{ub}, modification that is going to be applied to the
    entire data buffer when the chunk is "frozen".
    
    Allowed mask values and relative functions are:
    
    Custom fast modification:
    MASK_NONE   - Nothing (default)
    MASK_XOR    - Encrypt with XOR technique
    MASK_SWAP   - Encrypt by swapping hi/lo part of longs
    MASK_FAST   - A fast algorythm that includes some of the previous
                  methods
    
    To use XPK methods set CK_MASK to XPK_COMPRESSION_MODE then add these
    other tags:
    
    CK_XPK_METHOD, STRPTR - set it to a string pointer containing the 4
                            letter name of the XPK method
    CK_XPK_MODE, 0-100    - set it to the efficiency value for the
                            provided method. Default value is 50.
    
    Password  and  keys  are  not supported (and probably will never be) as
    decompression  is an automatic feature done while loading and cannot be
    blocked   by  any  custom  password  request.  For  not  critical  data
    protection  an  internal  password  is  used  for XPK methods needing a
    password (like IDEA or FEAL encryptors), but the chunk is automatically
    decompressed  and  decrypted  as the UDF file is loaded. If you want to
    make  your  data 100% protected you have to use XPK methods manually on
    the  data to be put in the chunk buffer and then manually decompress it
    once  loaded.  This  is  legal.  In  future a way to allow for "manual"
    packing/unpacking of chunk buffers may be added.
    
    See @{"what are metachunks" LINK "meta"} and @{"metachunk creation" LINK metachunk} section as well.
  
@endnode

@node meta "Metachunks"
  
                        @{b}What are metachunks?@{ub}
  
  Metachunk is an added feature of UDF. These are small, almost independant
  chunks,  which are part of a normal chunk. Metachunks are not part of the
  chunk  data  buffer,  but  are handled separately in linked list which is
  part  of  the  chunk.  Their  usage is very simple: they are meant to add
  additional  information  to the content of the chunk without modifing it.
  Let's  suppose  you have a picture compressed in a chunk. You may want to
  know  which  are the dimensions of such image, but being compressed (with
  whichever   algorythm  you  want)  you  have  to  recognize  its  format,
  decompress  it  and  then parse it to seek for the wanted information. At
  the  end  you may decide that that it is not of the dimensions you wanted
  or  that you just needed its dimensions and not the decoded image itself.
  This  is a big waste of resources and CPU power expecially if you have to
  do this over some pictures in the archive.
  
  Metachunk  can  be  used  to  store  the  above information in a separate
  location  which  can  be  easily  accessed. You just look for a metachunk
  which  has  the right ID (in this case ID_XY may be the right choice, but
  again  anything  you may recognize as being the right one is ok) and then
  just  extract  the width and height information from it without having to
  parse the chunk data even if the image data are already ready to be used,
  that is uncompressed.
  
  Another  usage  may  be  to  add information to a simple ASCII text, that
  being  in  ASCII  format  can't  contain  custom  formatting information.
  Through  metachunks you may store/obtain the size and font which the text
  is  supposed  to  be  written  with, or its colour, or the layer which it
  belongs  to  and  so on. The ASCII text will be available for any program
  that  cannot  make  any  or  very  limited  formatting on it (simple text
  editors  or  text viewers) but can be shown in its full glory by programs
  that   can   do   text   modifications   (word  processors,  DTP  or  DTV
  applications).
  
  Creating  and  handling  metachunks is very easy. Almost all hard work is
  done  automatically.  You  just  need  to  create them to add them to the
  current  chunk and to look for them, either by their ID and/or NUMBER, to
  get  a  pointer  to  the  required  metack  object  from  which  all  the
  information are available.
  
  See @{"Metachunk creation" LINK "metachunk"} section to know how to create a metachunk.
  
@endnode

@node metachunk "Metachunk creation"
  
                         @{b}Metachunk creation@{ub}
  
  Creating  a  metachunk is very similar to creating a normal chunk, though
  they  are  handled  in  a  very  different way internally. Moreover other
  differences  exists  when  trying  to get them and to access them. Please
  note  that  not  being  the  final  version  of  the module the functions
  provided  for  metachunks  can  change in future as may be their internal
  handling  and  accessing  mode. I'm considering whether to make metachunk
  data buffer 16 bit only large to spare a INT to be used for keeping track
  of the used part of the rounded buffer size.
  
  For  now metachunks have these differences with respect to normal chunks:
  ID  and NUMBER fields size are 16 bit only, while the data buffer size is
  32 bit. A metachunk can be looked by only its ID and number. The returned
  object is a complete metack object and the buffer data is in metack.data.
  All  metachunks are, eventually, compessed together with the normal chunk
  data and are automatically restored during loading process trasparently.
  
  As said all this may change in near future.
  
  To  create a metachunk you have first to @{"create a normal chunk" LINK "chunk"}. Then you
  can  add  a metachunk to it by using the pointer returned by the previous
  function  (pushlevel()  or  pushchunk()).  The  chunk  method to create a
  metachunk is called addmeta():
  
  @{b}addmeta(tags)@{ub} - tags is an array of tagitems that must include:
  
    CK_ID, as for normal chunk ID, but only the lower 16 bits are stored
    CK_NUMBER, a number from 0 to 65535
    CK_SIZE, a 32 bit value indicating the total size of the metachunk data
             buffer. Notice that this number will be rounded to the
             nearest multiple of 4.
    CK_BUFFER, a pointer to a memory area containing the data to be put in
               the metachunk. The data will be copied, so the original
               buffer may even be discarted after metachunk creation
               (i.e. static data).
               
    No other tags are recognized or used. The compression method is the
    same as that of the normal chunk which this metachunk belongs to.
  
  Other  metachunk  can be created this way. They'll all be linked together
  in a list.
  
  To  look  for  a particular metachunk in the list you have first to get a
  pointer   to   the   wanted   normal  chunk  (i.e.  through  the  use  of
  findchunk()/collectchunks()  methods).  Then  use this pointer to use the
  findmeta() function:
  
  @{b}findmeta(tags)@{ub} - allowed tags:
  
    CK_NUMBER, a number from 0 to 65535
    CK_ID, the required metachunk ID
    
    No  other  tags are recognized/used. The above tags can appear alone or
    together.  In  the  first  case  the  first metachunk with the required
    characteristic  will  be  returned  (or  NIL  if none is found). In the
    second  case  the  first  metachunk satisfing both the requires will be
    returned.  So  take care not to put two metachunks with the same ID and
    number  or  you  won't never have access to the second one through this
    function.
    
@endnode

@node hints "Some hints"
  
                              @{b}Some hints@{ub}
  
  Here some simple rules to make a simple but well organized hierarchical
  tree:
  
     I. Remember that the first level is one chunk only.
    II. Divide the chunks into levels.
   III. Consider assigning to parent chunk which work as container for lots
        of subchunk an ID like ID_FOLDER or ID_LIST and a meaningful
        NUMBER.
    IV. You may find useful putting some information about the subchunk list
        into the data buffer of such containers.
     V. Exploit metachunks to add additional information to the chunck 
        content.
    VI. The memory and file space waste for the creation of a chunk is very
        small, so do not spare them when trying to organize data in a neat
        way.
   VII. When looking for a particular set of chunks first look for their
        container chunk (ID_FOLDER or ID_LIST as in point III) and then
        scan the sublist of this chunk.
  VIII. Exploit the search by the number of subchunks when possible to
        highly restrict the number of folder sublists to scan.
    IX. Be careful that using XPK compressors on very small chunks (<200
        bytes) the resulting chunk may even be larger than uncompressed
        one. This is due to the fact that XPK adds additional information
        to the chunk content after compression.
     X. You are free (and encouraged) to use any legal (I stress that,
        LEGAL) way to obtain anything you want from this module. The most
        secure way to be sure any future enhancement will remain compatible
        with your application is to use ONLY the available documented API.
     
@endnode
